{"cells":[{"cell_type":"markdown","id":"5e7f2f2c-6302-40e9-b643-4556a6604ed0","metadata":{"id":"5e7f2f2c-6302-40e9-b643-4556a6604ed0"},"source":["# 이웃집 토토치 파이토치 : Day 2\n","---"]},{"cell_type":"markdown","id":"7e34878d-54a2-40de-ae3a-75866a8e2097","metadata":{"id":"7e34878d-54a2-40de-ae3a-75866a8e2097"},"source":["<div class=\"alert alert-info\">\n","    <p>📢 해당 게시물은 파이토치 공식 튜토리얼 중 <a href=\"https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html\">예제로 배우는 파이토치(PYTORCH) </a>와 파이토치(PYTORCH) 기본 익히기-<a href=\"https://tutorials.pytorch.kr/beginner/basics/tensorqs_tutorial.html\">텐서(Tensor)</a>, <a href=\"https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html#id13\">DATASET과 DATALOADER</a>, <a href=\"https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html\">Autograd</a> 그리고 토치비전 공식문서의 Example gallery-<a href=https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\">ILLUSTRATION OF TRANSFORMS</a>를 재구성하여 작성되었습니다.</p>\n","</div>"]},{"cell_type":"markdown","id":"9b8956f1-a5de-4cfb-8c38-b2b4b098b2e5","metadata":{"id":"9b8956f1-a5de-4cfb-8c38-b2b4b098b2e5"},"source":["#### 주요 키워드\n","- **Tensor**\n","- **Autograd**\n","- Dataset과 Dataloader\n","- Transform"]},{"cell_type":"markdown","id":"81fd1b58-3ada-46cf-a2c9-655692e3a2a6","metadata":{"id":"81fd1b58-3ada-46cf-a2c9-655692e3a2a6","tags":[],"toc-hr-collapsed":true},"source":["#### 목차\n","1. 탠서(TENSOR)\n","    1. 텐서(tensor) 초기화\n","    2. 텐서의 속성(Attribute)\n","    3. 텐서 연산(Operation)\n","    4. NumPy 변환(Bridge)\n","    5. [실습] Numpy to Tensor\n","2. Autograd\n","    1. Tensor, Function과 연산그래프(Computational graph)\n","    2. 변화도(Gradient) 계산하기\n","    3. 변화도 추적 멈추기\n","    4. 연산 그래프에 대한 추가 정보\n","    5. 선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱 (Jacobian Product)\n","    6. [실습] Backpropagation with Autograd\n","3. DATASET과 DATALOADER\n","    1. 데이터셋 불러오기\n","    2. 데이터셋을 순회하고 시각화하기\n","    3. 파일에서 사용자 정의 데이터셋 만들기\n","    4. DataLoader로 학습용 데이터 준비하기\n","    5. DataLoader를 통해 순회하기\n","4. 변형(TRANSFORM)\n","    1. ToTnesor()\n","    2. Lambda 변형(Transform)"]},{"cell_type":"markdown","id":"8d8cdc33-53ef-443e-967e-d221a4f8fbb5","metadata":{"id":"8d8cdc33-53ef-443e-967e-d221a4f8fbb5"},"source":["## 1. 텐서(TENSOR) \n","---"]},{"cell_type":"markdown","id":"6b4d8f21-377c-4aa0-941b-b7909cc2f2cf","metadata":{"id":"6b4d8f21-377c-4aa0-941b-b7909cc2f2cf"},"source":["텐서(tensor)는 배열(array)이나 행렬(matrix)과 매우 유사한 특수한 자료구조입니다.\n","PyTorch에서는 텐서를 사용하여 모델의 입력(input)과 출력(output), 그리고 모델의 매개변수들을 부호화(encode)합니다.\n","\n","텐서는 GPU나 다른 하드웨어 가속기에서 실행할 수 있다는 점만 제외하면 [NumPy](https://numpy.org)의 ndarray와 유사합니다.\n","실제로 텐서와 NumPy 배열(array)은 종종 동일한 내부(underly) 메모리를 공유할 수 있어 데이터를 복수할 필요가 없습니다. ([NumPy 변환(Bridge)](https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html#bridge-to-np-label) 참고)\n","텐서는 또한 ([Autograd](autogradqs_tutorial.html)장에서 살펴볼) 자동 미분(automatic differentiation)에 최적화되어 있습니다.\n","ndarray에 익숙하다면 Tensor API를 바로 사용할 수 있을 것입니다. 아니라면, 아래 내용을 함께 보시죠!"]},{"cell_type":"code","execution_count":null,"id":"84b21bbe-2e1f-4a0c-9863-cc7b9b4bf2b8","metadata":{"id":"84b21bbe-2e1f-4a0c-9863-cc7b9b4bf2b8"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","id":"33173f1c-e196-4f80-87f6-25adb3f7907e","metadata":{"id":"33173f1c-e196-4f80-87f6-25adb3f7907e"},"source":["### A. 텐서(tensor) 초기화\n","텐서는 여러가지 방법으로 초기화할 수 있습니다. 다음 예를 살펴보세요:"]},{"cell_type":"markdown","id":"3d02882d-b5c0-4f6a-979e-14484173f0f6","metadata":{"id":"3d02882d-b5c0-4f6a-979e-14484173f0f6"},"source":["#### a. 데이터로부터 직접(directly) 생성하기\n","데이터로부터 직접 텐서를 생성할 수 있습니다. 데이터의 자료형(data type)은 자동으로 유추합니다."]},{"cell_type":"code","execution_count":null,"id":"ecc80d93-e872-4cc8-9948-29e323d0cce8","metadata":{"id":"ecc80d93-e872-4cc8-9948-29e323d0cce8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647941765366,"user_tz":-540,"elapsed":5,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"f5c3a7ce-f153-492b-8dfb-a6a81e9fa7cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2],\n","        [3, 4]])"]},"metadata":{},"execution_count":17}],"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","x_data"]},{"cell_type":"markdown","id":"b7fcefc3-4830-4ad3-a74c-bd9e7976aa2f","metadata":{"id":"b7fcefc3-4830-4ad3-a74c-bd9e7976aa2f"},"source":["#### b. NumPy 배열로부터 생성하기\n","텐서는 NumPy 배열로 생성할 수 있습니다. (그 반대도 가능합니다 - [NumPy 변환(Bridge)](https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html#bridge-to-np-label) 참고)"]},{"cell_type":"code","execution_count":null,"id":"753be768-90dd-4806-9b63-dc9adec2736b","metadata":{"id":"753be768-90dd-4806-9b63-dc9adec2736b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647941768471,"user_tz":-540,"elapsed":6,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"9c205358-e1d7-43aa-c1be-8a8523f060b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2],\n","        [3, 4]])"]},"metadata":{},"execution_count":18}],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","x_np"]},{"cell_type":"markdown","id":"bcfd57ba-92e2-4ea9-8826-25196fdcb07d","metadata":{"id":"bcfd57ba-92e2-4ea9-8826-25196fdcb07d"},"source":["#### c. 다른 텐서로부터 생성하기\n","명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape), 자료형(datatype))을 유지합니다."]},{"cell_type":"code","execution_count":null,"id":"6100365e-0302-4ad2-8623-0b250116ae13","metadata":{"id":"6100365e-0302-4ad2-8623-0b250116ae13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647934857858,"user_tz":-540,"elapsed":481,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"4b53c43a-d1bf-49bd-cd23-8042251ae421"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.8745, 0.2901],\n","        [0.3043, 0.1936]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","id":"1676896b-fa1e-46f2-afee-3e9da2457e0b","metadata":{"id":"1676896b-fa1e-46f2-afee-3e9da2457e0b"},"source":["#### d. 무작위(random) 또는 상수(constant) 값을 사용하기\n","`shape`은 텐서의 차원(dimension)을 나타내는 튜플(tuple)로, 아래 함수들에서는 출력 텐서의 차원을 결정합니다."]},{"cell_type":"code","execution_count":null,"id":"476f0e06-3c50-4d4c-bd47-6d5b5810e768","metadata":{"id":"476f0e06-3c50-4d4c-bd47-6d5b5810e768","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647934871623,"user_tz":-540,"elapsed":462,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"4bac4036-309e-4355-ee6f-8ef6d1556f34"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.8884, 0.1649, 0.8549],\n","        [0.5514, 0.5685, 0.6841]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","id":"6f14c531-430a-45e7-818d-278876cc7735","metadata":{"id":"6f14c531-430a-45e7-818d-278876cc7735"},"source":["### B. 텐서의 속성(Attribute)\n","텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냅니다."]},{"cell_type":"code","execution_count":null,"id":"c0d18f60-2823-47ee-958f-b3de620f5357","metadata":{"id":"c0d18f60-2823-47ee-958f-b3de620f5357","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647934889385,"user_tz":-540,"elapsed":379,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"c9e7f3c6-b64c-43b1-cc9c-bb5a2f4e1921"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3,4)\n","\n","############################\n","# 밑줄 친 곳을 채워주세요! #\n","############################\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","id":"dc8ec2e3-b2e1-406c-a654-59cbc060aa4e","metadata":{"id":"dc8ec2e3-b2e1-406c-a654-59cbc060aa4e"},"source":["### C. 텐서 연산(Operation)\n","전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링(random sampling) 등, 100가지 이상의 텐서 연산들을 [여기](https://pytorch.org/docs/stable/torch.html)에서 확인할 수 있습니다.\n","\n","각 연산들은 (일반적으로 CPU보다 빠른) GPU에서 실행할 수 있습니다. Colab을 사용한다면, Edit > Notebook Settings 에서 GPU를 할당할 수 있습니다.\n","\n","기본적으로 텐서는 CPU에 생성됩니다. .to 메소드를 사용하면 (GPU의 가용성(availability)을 확인한 뒤) GPU로 텐서를 명시적으로 이동할 수 있습니다. 장치들 간에 큰 텐서들을 복사하는 것은 시간과 메모리 측면에서 비용이 많이든다는 것을 기억하세요!"]},{"cell_type":"code","execution_count":null,"id":"7a2f8fb6-b044-4cbd-bcdc-80c1ec7a82cb","metadata":{"id":"7a2f8fb6-b044-4cbd-bcdc-80c1ec7a82cb"},"outputs":[],"source":["# GPU가 존재하면 텐서를 이동합니다\n","if torch.cuda.is_available():\n","    tensor = tensor.to('cuda')\n","    print(tensor)"]},{"cell_type":"markdown","id":"40866338-5f65-4d75-a97b-63457e90cff9","metadata":{"id":"40866338-5f65-4d75-a97b-63457e90cff9"},"source":["목록에서 몇몇 연산들을 시도해보세요. NumPy API에 익숙하다면 Tensor API를 사용하는 것은 식은 죽 먹기라는 것을 알게 되실 겁니다."]},{"cell_type":"markdown","source":["Q. 텐서를 사용하면 어떤 점이 좋을까요??\n","\n","👉  메모리 공간을 효율적으로 사용할 수 있기 때문에 속도가 향상된다."],"metadata":{"id":"ZYzzUHoX4UCY"},"id":"ZYzzUHoX4UCY"},{"cell_type":"markdown","id":"631fdca2-aab4-4e2d-b6bf-9062338d9101","metadata":{"id":"631fdca2-aab4-4e2d-b6bf-9062338d9101"},"source":["#### a. NumPy식의 표준 인덱싱과 슬라이싱"]},{"cell_type":"code","execution_count":null,"id":"535e7a6a-6b64-45c0-ad27-99b8e26584e6","metadata":{"id":"535e7a6a-6b64-45c0-ad27-99b8e26584e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647934973421,"user_tz":-540,"elapsed":445,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"a2a25404-c470-4fd0-94b5-b5646ebd3802"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row:  tensor([1., 1., 1., 1.])\n","First column:  tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","print('First row: ',tensor[0])\n","print('First column: ', tensor[:, 0])\n","print('Last column:', tensor[..., -1])\n","tensor[:,1] = 0\n","print(tensor)"]},{"cell_type":"code","source":["data = [[1, 2, 3],\n","         [4, 5, 6],\n","         [7, 8, 9]]\n","t_data = torch.tensor(data)\n","\n","############################\n","# 밑줄 친 곳을 채워주세요! #\n","############################\n","\n","print('2번째 행 출력: ',t_data[1])\n","print('t_data에서 5를 출력: ', t_data[1,1])\n","print('마지막 column 출력:', t_data[:,-1])\n","print('3번째 column을 0으로 만들기:')\n","t_data[:,2] = 0\n","print(t_data)"],"metadata":{"id":"RVgYMPsUSaiQ"},"id":"RVgYMPsUSaiQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0f5b5a86-18ed-4af1-be4a-91717895d609","metadata":{"id":"0f5b5a86-18ed-4af1-be4a-91717895d609"},"source":["텐서 합치기 `torch.cat`을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있습니다. torch.cat 과 미묘하게 다른 또 다른 텐서 결합 연산인 [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)도 참고해보세요."]},{"cell_type":"code","execution_count":null,"id":"c47ef522-8161-4508-991f-32fd0db49ea8","metadata":{"id":"c47ef522-8161-4508-991f-32fd0db49ea8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647935008870,"user_tz":-540,"elapsed":391,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"3b0bb671-9880-48a8-b77f-ea23ea549bf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"]}],"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"]},{"cell_type":"markdown","id":"38f8542e-a54a-40b8-982c-94723f164257","metadata":{"id":"38f8542e-a54a-40b8-982c-94723f164257"},"source":["#### b. 산술 연산(Arithmetic operations)"]},{"cell_type":"code","source":["# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(tensor)\n","torch.matmul(tensor, tensor.T, out=y3) # torch.matmul 함수는 무엇을 의미하는 가?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4YZFj3vle89","executionInfo":{"status":"ok","timestamp":1647935095846,"user_tz":-540,"elapsed":392,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"088244cd-2e92-4f58-b761-a7dd75a7a3f4"},"id":"G4YZFj3vle89","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":null,"id":"431105fb-a68f-4d58-8550-03e0cae85986","metadata":{"id":"431105fb-a68f-4d58-8550-03e0cae85986","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647935101831,"user_tz":-540,"elapsed":422,"user":{"displayName":"문소정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10243601240582310320"}},"outputId":"8362dafd-7314-49d1-f111-63de75631679"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])"]},"metadata":{},"execution_count":12}],"source":["# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","\n","z3 = torch.rand_like(tensor)\n","torch.mul(tensor, tensor, out=z3) # torch.mul 함수가 의미하는 것은 무엇인가?"]},{"cell_type":"markdown","source":["- `torch.matmul(input, other, out=None)` : 두 tensor를 행렬 곱을 반환(input과 other의 차원에 따라 결과값이 달라지므로 [링크](https://pytorch.org/docs/stable/generated/torch.matmul.html) 참조)\n","- `torch.rand_like` : 평균 0, 분산 1인 정규분포의 난수로 채워진 input과 같은 사이즈로 tensor를 반환\n","- `torch.mul(input, other, out=None)` : input과 other을 곱해서 결과 tensor를 반환"],"metadata":{"id":"FtcrhrcJCxQt"},"id":"FtcrhrcJCxQt"},{"cell_type":"markdown","source":["Q. 행렬 곱(matrix multiplication)과 요소별 곱(element-wise product)의 차이점이 뭘까요??\n","\n","👉 행렬 곱은 선형변환이나 내적등의 해석이 가능하고, 요소곱은 말그대로 그냥 요소별 곱이다[[예제](https://bskyvision.com/895)]"],"metadata":{"id":"k57ZJ-EQ4djE"},"id":"k57ZJ-EQ4djE"},{"cell_type":"markdown","id":"ac408566-f305-4a2e-bfc6-6a4c979d4d11","metadata":{"id":"ac408566-f305-4a2e-bfc6-6a4c979d4d11"},"source":["**단일-요소(single-element) 텐서**의 모든 값을 하나로 집계(aggregate)하여 요소가 하나인 텐서의 경우, `item()`을 사용하여 Python 숫자 값으로 변환할 수 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"57b5ea67-3e56-4eb6-9234-cf081999185d","metadata":{"id":"57b5ea67-3e56-4eb6-9234-cf081999185d"},"outputs":[],"source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","id":"a096f157-2ca6-476f-9d2d-ed5add07fe2e","metadata":{"id":"a096f157-2ca6-476f-9d2d-ed5add07fe2e"},"source":["**바꿔치기(in-place) 연산** 연산 결과를 피연산자(operand)에 저장하는 연산을 바꿔치기 연산이라고 부르며,  `_` 접미사를 갖습니다. 예를 들어, x.copy_(y) 나 x.t_() 는 x 를 변경합니다."]},{"cell_type":"code","source":["print(tensor, \"\\n\")\n","tensor.add_(5)\n","print(tensor)"],"metadata":{"id":"CF6hwyY9HEvr"},"id":"CF6hwyY9HEvr","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c163fa49-91ea-42b1-8e5c-358b0dde0f3f","metadata":{"id":"c163fa49-91ea-42b1-8e5c-358b0dde0f3f"},"source":["<div class=\"alert alert-info\">\n","    <b>📌NOTE</b>\n","    <div>바꿔치기 연산은 메모리를 일부 절약하지만, 기록(history)이 즉시 삭제되어 도함수(derivative) 계산에 문제가 발생할 수 있습니다. 따라서, 사용을 권장하지 않습니다.</div>\n","</div>"]},{"cell_type":"markdown","source":["**텐서차원 축소/확장(Squeeze/Unsqueeze) 연산**"],"metadata":{"id":"uwF1_AIc5aX2"},"id":"uwF1_AIc5aX2"},{"cell_type":"code","source":["# Squeeze/Unsqueeze\n","x = torch.rand((1,1,3,4))\n","y = x.squeeze()\n","print(y.size())\n","print(y.unsqueeze(1).size())"],"metadata":{"id":"vXnIzFkL5hg_"},"id":"vXnIzFkL5hg_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q. Squeeze와 Unsqueeze는 어떤 경우에 사용 될까요? 자유롭게 생각해보죠!\n","\n","👉 텐서를 연산할 경우, 차원을 맞추려고 사용한다."],"metadata":{"id":"PeKuzg4W5jJe"},"id":"PeKuzg4W5jJe"},{"cell_type":"markdown","source":["**스태킹(Stacking)**"],"metadata":{"id":"-2oPEFNE5k6g"},"id":"-2oPEFNE5k6g"},{"cell_type":"code","source":["x = torch.FloatTensor([1, 4])\n","y = torch.FloatTensor([2, 5])\n","z = torch.FloatTensor([3, 6])\n","\n","print(torch.stack([x, y, z]))"],"metadata":{"id":"ZeruRJh05nNV"},"id":"ZeruRJh05nNV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"acf19d2e-2adf-42ec-b312-e37c4c942710","metadata":{"id":"acf19d2e-2adf-42ec-b312-e37c4c942710"},"source":["### D. NumPy 변환(Bridge)\n","CPU 상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다."]},{"cell_type":"markdown","id":"04d1de7a-b49f-4b69-96e9-c4b4bc48731e","metadata":{"id":"04d1de7a-b49f-4b69-96e9-c4b4bc48731e"},"source":["#### a. 텐서를 NumPy 배열로 변환하기"]},{"cell_type":"code","execution_count":null,"id":"0380fad1-29cc-42aa-99e8-08d324e4fd4c","metadata":{"id":"0380fad1-29cc-42aa-99e8-08d324e4fd4c"},"outputs":[],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","id":"49599d36-5081-4c2c-91cd-2dcf2909dd35","metadata":{"id":"49599d36-5081-4c2c-91cd-2dcf2909dd35"},"source":["텐서의 변경 사항이 NumPy 배열에 반영됩니다."]},{"cell_type":"code","execution_count":null,"id":"f02c53b2-9c6b-4db8-8503-ae7070281716","metadata":{"id":"f02c53b2-9c6b-4db8-8503-ae7070281716"},"outputs":[],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","id":"e1be91ad-d832-438b-a538-037ccaead865","metadata":{"id":"e1be91ad-d832-438b-a538-037ccaead865"},"source":["#### b. NumPy 배열을 텐서로 변환하기"]},{"cell_type":"code","execution_count":null,"id":"7c9e1f5b-02d1-4aa6-8af5-ea52d60e7f69","metadata":{"id":"7c9e1f5b-02d1-4aa6-8af5-ea52d60e7f69"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)\n","t"]},{"cell_type":"markdown","id":"d5fc0d4d-8ff7-4b16-80af-1632f223aa9f","metadata":{"id":"d5fc0d4d-8ff7-4b16-80af-1632f223aa9f"},"source":["NumPy 배열의 변경 사항이 텐서에 반영됩니다."]},{"cell_type":"code","execution_count":null,"id":"eb1c0529-b952-488d-8420-f4602c4e6e42","metadata":{"id":"eb1c0529-b952-488d-8420-f4602c4e6e42"},"outputs":[],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","id":"c649cc4d-39c9-4676-aa96-ad39a9a4a84d","metadata":{"id":"c649cc4d-39c9-4676-aa96-ad39a9a4a84d"},"source":["### E. [실습] Numpy to Tensor"]},{"cell_type":"markdown","id":"2a9a591c-019f-4083-b5e0-a75ba5e09d6a","metadata":{"id":"2a9a591c-019f-4083-b5e0-a75ba5e09d6a"},"source":["본질적으로, PyTorch에는 두가지 주요한 특징이 있습니다:\n","\n","- NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n","- 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)\n","\n","이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 $y=\\sin(x)$ 에 근사(fit)하는 문제를 다뤄보겠습니다. 신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리(Euclidean distance)를 최소화하여 임의의 값을 근사할 수 있도록 경사하강법(gradient descent)을 사용하여 학습하겠습니다."]},{"cell_type":"markdown","id":"fb231346-9453-4fed-883a-2984bb42b219","metadata":{"id":"fb231346-9453-4fed-883a-2984bb42b219"},"source":["#### a. Numpy"]},{"cell_type":"markdown","id":"09744d29-bdc0-4577-b8dc-e42e93888f6c","metadata":{"id":"09744d29-bdc0-4577-b8dc-e42e93888f6c"},"source":["PyTorch를 소개하기 전에, 먼저 `NumPy`를 사용하여 신경망을 구성해보겠습니다.\n","\n","NumPy는 n-차원 배열 객체와 이러한 배열들을 조작하기 위한 다양한 함수들을 제공합니다. Num\n","Py는 과학 분야의 연산을 위한 포괄적인 프레임워크(generic framework)입니다. \n","\n","NumPy는 연산 그래프(computation graph)나 딥러닝, 변화도(gradient)에 대해서는 알지 못합니다. 하지만 NumPy 연산을 사용하여 신경망의 순전파 단계와 역전파 단계를 직접 구현함으로써, 3차 다항식이 사인(sine) 함수에 근사하도록 만들 수 있습니다.\n","\n","✅ 구현해야 하는 사항 : Numpy를 이용한 순전파, 손실, 역전파\n","- 이때, 입력값의 범위는 `-π ~ π` 이다. 따라서 구현되는 순전파 함수는 삼차다항식의 꼴이 된다.\n","- 이때, 손실은 예측값과 실제값의 유클리드 거리로 구한다."]},{"cell_type":"code","execution_count":null,"id":"b2fd4ac5-a369-435a-9e68-c475010d47dc","metadata":{"id":"b2fd4ac5-a369-435a-9e68-c475010d47dc"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","# 무작위로 입력과 출력 데이터를 생성합니다\n","x = np.linspace(-math.pi, math.pi, 2000)\n","y = np.sin(x)\n","\n","# 무작위로 가중치를 초기화합니다\n","a = np.random.randn()\n","b = np.random.randn()\n","c = np.random.randn()\n","d = np.random.randn()\n","\n","print(f'------------------ init ------------------')\n","print(f'x({len(x)}) : {x}')\n","print(f'y({len(y)}) : {y}')\n","print(f'가중치 초기값 : ')\n","print(f'  a : {a}')\n","print(f'  b : {b}')\n","print(f'  c : {c}')\n","print(f'  d : {d}')\n","print(f'------------------------------------------')\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 예측값 y를 계산합니다\n","    # y = a + b x + c x^2 + d x^3\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 손실(loss)을 계산하고 출력합니다\n","    loss = np.square(y_pred - y).sum()\n","    if t % 100 == 99:\n","        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n","\n","    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = (grad_y_pred * x ** 3).sum()\n","\n","    # 가중치를 갱신합니다.\n","    a -= learning_rate * grad_a\n","    b -= learning_rate * grad_b\n","    c -= learning_rate * grad_c\n","    d -= learning_rate * grad_d\n","    \n","print(f'------------------------------------------')\n","print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"]},{"cell_type":"code","execution_count":null,"id":"bdca8ad5-edcd-4189-9d78-3a42d995fefc","metadata":{"id":"bdca8ad5-edcd-4189-9d78-3a42d995fefc"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def show(x, y, y_pred):\n","    fig = plt.figure(figsize=(10, 4))\n","\n","    plt.plot(x, y, color='red', label=\"real\")\n","    plt.plot(x, y_pred, color='blue', label=\"prediction\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"]},{"cell_type":"markdown","id":"9f417d32-37d0-4773-9f65-3af636582c2d","metadata":{"id":"9f417d32-37d0-4773-9f65-3af636582c2d"},"source":["<div class=\"alert alert-warning\">\n","    <p><b>Q. `grad_y_pred`를 계산하는 수식에서 왜 2를 곱하는 이유는 무엇인가?</b></p>\n","    <p>👉  loss function에 대한 미분값 계산 : (y_pred - y)^2 → 2 * (y_pred - y)</p>\n","</div>"]},{"cell_type":"markdown","id":"80b79538-48ca-473b-86a7-fa8841712c8a","metadata":{"id":"80b79538-48ca-473b-86a7-fa8841712c8a"},"source":["<div class=\"alert alert-warning\">\n","    <p><b>Q. 각 가중치에 대한 gradient를 구하는 식은 어떻게 유도되었는가?</b></p>\n","    <p>👉 loss function의 미분값을 계산해서 모두 더해준다.</p>\n","</div>"]},{"cell_type":"markdown","id":"df2aa280-6127-41bf-a5e7-d91daa8c9ed8","metadata":{"id":"df2aa280-6127-41bf-a5e7-d91daa8c9ed8"},"source":["<div class=\"alert alert-warning\">\n","    <p><b>Q. 다음 함수들의 기능은 무엇인가?</b>\n","    <a href=\"https://numpy.org/doc/stable/index.html\">hint</a></p>\n","    <div>👉 numpy.linespace : 정해진 범위 내에서 균일간 간격의 숫자를 반환함<br>\n","             👉 numpy.sin : sin 함수 구현<br>\n","             👉 numpy.square : 입력값의 제곱 값 반환</div>\n","</div>"]},{"cell_type":"markdown","id":"fb5c00bf-14f5-4975-8de7-52e86a9bb9ac","metadata":{"id":"fb5c00bf-14f5-4975-8de7-52e86a9bb9ac"},"source":["#### b. Tensor"]},{"cell_type":"markdown","id":"2c25f57c-74f4-4989-a8c6-0d1e7b1b0c0d","metadata":{"id":"2c25f57c-74f4-4989-a8c6-0d1e7b1b0c0d"},"source":["NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없습니다. 현대의 심층 신경망에서 GPU는 종종 [50배 또는 그 이상](https://github.com/jcjohnson/cnn-benchmarks) 의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.\n","\n","이번에는 PyTorch의 가장 핵심적인 개념인 **텐서(Tensor)** 에 대해서 알아보겠습니다. PyTorch 텐서(Tensor)는 개념적으로 NumPy 배열과 동일합니다. \n","- 텐서(Tensor)는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 다양한 기능들을 제공합니다.\n","- NumPy 배열처럼 PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구입니다. \n","- 텐서는 연산 그래프와 변화도를 추적할 수도 있지만, 과학적 연산을 위한 일반적인 도구로도 유용합니다.\n","- 또한 NumPy와는 다르게, PyTorch 텐서는 GPU를 사용하여 수치 연산을 가속할 수 있습니다. PyTorch 텐서를 GPU에서 실행하기 위해서는 단지 적절한 장치를 지정해주기만 하면 됩니다.\n","\n","위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계구현하되 입력/출력/가중치 값을 PyTorch 텐서를 사용하여 정의해 봅시다. \n","\n","✅ 구현해야 하는 사항 : 이전 구현에서 Numpy를 torch.Tensor로 대체한다."]},{"cell_type":"code","execution_count":null,"id":"5b6c9266-1b2c-42cc-b405-ffe5b560ba42","metadata":{"id":"5b6c9266-1b2c-42cc-b405-ffe5b560ba42"},"outputs":[],"source":["import torch\n","import math\n","\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n","\n","# 무작위로 입력과 출력 데이터를 생성합니다\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# 무작위로 가중치를 초기화합니다\n","a = torch.randn((), device=device, dtype=dtype)\n","b = torch.randn((), device=device, dtype=dtype)\n","c = torch.randn((), device=device, dtype=dtype)\n","d = torch.randn((), device=device, dtype=dtype)\n","\n","print(f'------------------ init ------------------')\n","print(f'x({len(x)}, {x.__class__}) : {x}')\n","print(f'y({len(y)}, {y.__class__}) : {y}')\n","print(f'가중치 초기값 : ')\n","print(f'  a({a.__class__}) : {a}')\n","print(f'  b({b.__class__}) : {b}')\n","print(f'  c({c.__class__}) : {c}')\n","print(f'  d({d.__class__}) : {d}')\n","print(f'------------------------------------------')\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 예측값 y를 계산합니다\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 손실(loss)을 계산하고 출력합니다\n","    loss = (y_pred - y).pow(2).sum().item()\n","    if t % 100 == 99:\n","        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n","\n","    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_a = grad_y_pred.sum()\n","    grad_b = (grad_y_pred * x).sum()\n","    grad_c = (grad_y_pred * x ** 2).sum()\n","    grad_d = (grad_y_pred * x ** 3).sum()\n","\n","    # 가중치를 갱신합니다.\n","    a -= learning_rate * grad_a\n","    b -= learning_rate * grad_b\n","    c -= learning_rate * grad_c\n","    d -= learning_rate * grad_d\n","\n","print(f'------------------------------------------')\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n","\n","show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"]},{"cell_type":"markdown","id":"eb7ba389-11f0-406b-876c-87fa24bdb3be","metadata":{"id":"eb7ba389-11f0-406b-876c-87fa24bdb3be"},"source":["<div class=\"alert alert-warning\">\n","    <p><b>Q. 파이토치 Tensor가 Numpy에 비하여 가지는 장점이 무엇인가? 위의 문장들을 복사하지 말고 자신만의 표현으로 적어보자.</b></p>\n","    <p>👉 Tensor는 수치 계산에 있어 GPU를 사용 할 수 있기 때문에 Numpy보다 연산 속도가 빠르다.</p>\n","</div>"]},{"cell_type":"markdown","id":"f2e95c5e-71c5-4441-a558-dd1fd9f9e4bf","metadata":{"id":"f2e95c5e-71c5-4441-a558-dd1fd9f9e4bf"},"source":["## 2. Autograd\n","---"]},{"cell_type":"markdown","id":"65780702-4ad4-444d-87d6-d0e34b8787b7","metadata":{"id":"65780702-4ad4-444d-87d6-d0e34b8787b7"},"source":["신경망을 학습할 때 가장 자주 사용되는 알고리즘은 **역전파* 입니다. 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 **변화도(gradient)**에 따라 조정됩니다.\n","\n","이러한 변화도를 계산하기 위해 PyTorch에는 `torch.autograd`라고 불리는 **자동 미분 엔진**이 내장되어 있습니다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원합니다.\n","\n","입력 `x`, 매개변수 `w` 와 `b` , 그리고 일부 손실 함수가 있는 가장 간단한 단일 계층 신경망을 가정하겠습니다. PyTorch에서는 다음과 같이 정의할 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"0ab002f5-707e-4e5b-8888-137a06eca942","metadata":{"id":"0ab002f5-707e-4e5b-8888-137a06eca942"},"outputs":[],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"markdown","source":["### EX) 간단 이미지 분류 모델 학습"],"metadata":{"id":"AalgqMdATBAC"},"id":"AalgqMdATBAC"},{"cell_type":"code","source":["# 동일한 출력을 얻기 위해서 random seed 값 설정\n","random_seed = 99\n","torch.manual_seed(random_seed)"],"metadata":{"id":"riuUmyD3TWPZ"},"id":"riuUmyD3TWPZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### a. 데이터 준비하기"],"metadata":{"id":"ZvQ2OKoRTdm7"},"id":"ZvQ2OKoRTdm7"},{"cell_type":"code","source":["model = torch.nn.Sequential(\n","    torch.nn.Linear(6, 1),\n","    torch.nn.Flatten()\n",")\n","data = torch.rand(2, 6)\n","labels = torch.ones(2, 1)\n","labels"],"metadata":{"id":"LPXuM6DMTWNh"},"id":"LPXuM6DMTWNh","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["랜덤으로 설정된 label 값은 ([1.], [1.])입니다."],"metadata":{"id":"fqh2_PEwTp5L"},"id":"fqh2_PEwTp5L"},{"cell_type":"markdown","source":["Q. 각자 그림을 그리면서 이해하시는 것을 추천드립니다!\n","\n","👉 (해답은 토론 시간에 공유해볼게요))"],"metadata":{"id":"zjcmlI70TmX5"},"id":"zjcmlI70TmX5"},{"cell_type":"markdown","source":["![](https://github.com/AIFFEL-GN-2nd/TotochTeam1/raw/6cdd13f3dd147d3d4542b048ab022dedd99242b1/day_1/img/linear.jpg)"],"metadata":{"id":"15zgRG-Kx96F"},"id":"15zgRG-Kx96F"},{"cell_type":"markdown","source":["#### b. 순전파 (Forward Propagation)"],"metadata":{"id":"QnfztZxCTrlP"},"id":"QnfztZxCTrlP"},{"cell_type":"code","source":["# 순전파 단계(forward pass)\n","prediction = model(data) \n","prediction"],"metadata":{"id":"giWcbRF2TWJv"},"id":"giWcbRF2TWJv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["순전파 단계에서는 입력(input) 데이터를 모델의 각 층(layer)에 통과시켜 예측값(prediction)을 생성합니다.\n","모델의 output으로 나온 pred 값은 ([0.3425], [0.2512])입니다."],"metadata":{"id":"jN6zM-yrTzkP"},"id":"jN6zM-yrTzkP"},{"cell_type":"code","source":["layer = model[0]\n","print(f'Result: y = {layer.bias.item()} + {layer.weight[:, 0].item()} x + {layer.weight[:, 1].item()"],"metadata":{"id":"OpzZhKNTTWH0"},"id":"OpzZhKNTTWH0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델의 초기 weight와 bias를 확인 해보시길 바랍니다.\n","손실함수와 옵티마이저를 사용해서 이 값들을 변경할 것입니다."],"metadata":{"id":"C1KySfNnT2ma"},"id":"C1KySfNnT2ma"},{"cell_type":"markdown","source":["#### c. 손실함수 (loss) 계산하기"],"metadata":{"id":"AVJdeQpvT31y"},"id":"AVJdeQpvT31y"},{"cell_type":"code","source":["loss = (prediction - labels).sum() \n","print(loss)\n","loss.backward() # 역전파 단계(backward pass)"],"metadata":{"id":"OqNYcwv4TWFP"},"id":"OqNYcwv4TWFP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 모델의 예측값(prediction)과 그에 해당하는 정답(label)의 차이를 합하여 오차(error, 손실(loss) )를 계산하였습니다. 손실함수를 구하는 방법은 다양하지만 이번글에서는 간단하게 차이값으로만 구하도록 하겠습니다.\n","\n","- 오차 텐서(error tensor)에 .backward() 를 호출하면 역전파가 시작되며, 그 다음 Autograd가 매개변수(parameter)의 .grad 속성(attribute)에, 모델의 각 매개변수에 대한 변화도(gradient)를 계산하고 저장합니다."],"metadata":{"id":"PXC0Huk8T9XI"},"id":"PXC0Huk8T9XI"},{"cell_type":"markdown","source":["Q. loss의 결과는 -1.4063이 나왔습니다, 직접 labels 값과 pred 값을 비교해서 계산해봅시다!\n","\n","👉 0.3425 + 0.2512 - 2 = -1.4063"],"metadata":{"id":"hyJD2pG4UA5b"},"id":"hyJD2pG4UA5b"},{"cell_type":"markdown","source":["#### d. 옵티마이저 설정"],"metadata":{"id":"rQsxK5CNUCmC"},"id":"rQsxK5CNUCmC"},{"cell_type":"code","source":["optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"],"metadata":{"id":"OStze2KwTWDn"},"id":"OStze2KwTWDn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 학습율(learning rate) 0.01과 모멘텀(momentum) 0.9를 갖는 SGD로 옵티마이즈를 설정하였으며, 옵티마이저(optimizer)에 모델의 모든 매개변수(models.parameters())를 등록합니다."],"metadata":{"id":"Ucry6VlgUKd8"},"id":"Ucry6VlgUKd8"},{"cell_type":"markdown","source":["#### e. 경사하강법(gradient descent)"],"metadata":{"id":"kDKRNFT9UL7S"},"id":"kDKRNFT9UL7S"},{"cell_type":"code","source":["optim.step()"],"metadata":{"id":"RVJlXVO4TWB3"},"id":"RVJlXVO4TWB3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["step 을 호출하여 경사하강법(gradient descent)을 시작합니다. 옵티마이저는 .grad 에 저장된 기울기(gradient)에 따라 각 매개변수를 조정(adjust)합니다."],"metadata":{"id":"jeVvVQGdUhqg"},"id":"jeVvVQGdUhqg"},{"cell_type":"code","source":["layer = model[0]\n","print(f'Result: y = {layer.bias.item()} + {layer.weight[:, 0].item()} x + {layer.weight[:, 1].item()} x^2 + {layer.weight[:, 2].item()} x^3')"],"metadata":{"id":"fMwT-u3WTWAP"},"id":"fMwT-u3WTWAP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = model(data) # 순전파 단계(forward pass)\n","prediction"],"metadata":{"id":"UYQ6XJKSTV-O"},"id":"UYQ6XJKSTV-O","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### f. 실습 문제\n","\n","다음과 같은 연산 그래프를 직접 구현하면서 Autograd를 이해해보는 시간을 갖겠습니다.\n","이번 실습에서는 backpop의 미분 값이 어떻게 계산되는 지 살펴볼 것입니다.\n","x는 초기 값으로 2x2의 1로 채워진 행렬을 사용할 것입니다.\n","\n","![](https://github.com/AIFFEL-GN-2nd/TotochTeam1/raw/6cdd13f3dd147d3d4542b048ab022dedd99242b1/day_1/img/ex1.jpg)"],"metadata":{"id":"WQ7o75jjUlGQ"},"id":"WQ7o75jjUlGQ"},{"cell_type":"code","source":["x = torch.ones(2, 2)\n","print(x)\n","print(x.requires_grad)\n","print('-'*10)\n","\n","x = torch.ones(2, 2, requires_grad=True)\n","print(x)\n","print(x.requires_grad)"],"metadata":{"id":"GEWx6UsZTV5z"},"id":"GEWx6UsZTV5z","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q. requires_grad는 어떤 역할을 하는 파라미터일까요?\n","\n","👉 (여기에 답을 입력해 주세요)"],"metadata":{"id":"UYLXxjC-UwAB"},"id":"UYLXxjC-UwAB"},{"cell_type":"code","source":["############################\n","# 밑줄 친 곳을 채워주세요! #\n","############################\n","\n","y = x + 2\n","print(y)\n","print(y.requires_grad)"],"metadata":{"id":"OiQOPBF7UvR9"},"id":"OiQOPBF7UvR9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y.grad_fn)"],"metadata":{"id":"slVN3BsRUvQI"},"id":"slVN3BsRUvQI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################\n","# 밑줄 친 곳을 채워주세요! #\n","############################\n","\n","z = y * y * 3\n","out = z.mean()\n","print(z)\n","print(out)"],"metadata":{"id":"hgUX0ZP_UvNf"},"id":"hgUX0ZP_UvNf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.ones(5)  # input tensor\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"],"metadata":{"id":"Wwn2_lqTUvIy"},"id":"Wwn2_lqTUvIy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Gradient, 경사/기울기\n","- 역전파를 해보겠습니다. out.backward()과 out.backward(torch.Tensor([1.0]))은 동일하게 동작합니다."],"metadata":{"id":"-j5PFhJLU_j4"},"id":"-j5PFhJLU_j4"},{"cell_type":"code","source":["out.backward()"],"metadata":{"id":"vtadjvjdUvGE"},"id":"vtadjvjdUvGE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x.grad)"],"metadata":{"id":"cy45GFSgVFij"},"id":"cy45GFSgVFij","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Q. requires_grad = True를 하면 무슨 일이 일어나길래, grad()를 호출하면 바로 미분값을 합산해줄까?\n","\n","👉 [링크참조](https://www.youtube.com/watch?v=3Kb0QS6z7WA&feature=youtu.be)"],"metadata":{"id":"YLVMBiYdU7XG"},"id":"YLVMBiYdU7XG"},{"cell_type":"markdown","id":"07aa2e7d-b21c-4a6e-90c1-d67edcc28f72","metadata":{"id":"07aa2e7d-b21c-4a6e-90c1-d67edcc28f72"},"source":["### A. Tensor, Function과 연산그래프(Computational graph)"]},{"cell_type":"markdown","id":"f9705f65-99a2-4144-a6a5-79c971124881","metadata":{"id":"f9705f65-99a2-4144-a6a5-79c971124881"},"source":["이 코드는 다음의 **연산 그래프** 를 정의합니다:\n","\n","![](https://i.ibb.co/HFpgNcF/computational-graph.png)\n","\n","이 신경망에서, ``w`` 와 ``b`` 는 최적화를 해야 하는 **매개변수** 입니다. 따라서\n","이러한 변수들에 대한 손실 함수의 변화도를 계산할 수 있어야 합니다. 이를 위해서 해당 텐서에\n","``requires_grad`` 속성을 설정합니다.\n","\n","<div class=\"alert alert-info\">\n","    <b>📌NOTE</b>\n","    <div><b><i>requires_grad</i></b>의 값은 텐서를 생성할 때 설정하거나, 나중에 <b><i>x.requires_grad_(True)</i></b> 메소드를 사용하여 나중에 설정할 수도 있습니다.</div>\n","</div>\n","\n","연산 그래프를 구성하기 위해 텐서에 적용하는 함수는 사실 ``Function`` 클래스의 객체입니다. 이 객체는 *순전파* 방향으로 함수를 계산하는 방법과, *역방향 전파* 단계에서 도함수(derivative)를 계산하는 방법을 알고 있습니다. 역방향 전파 함수에 대한 참조(reference)는 텐서의 ``grad_fn`` 속성에 저장됩니다. ``Function`` 에 대한 자세한 정보는 [이 문서](https://pytorch.org/docs/stable/autograd.html#function)에서 찾아볼 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"cb880382-83f8-4eee-8dcc-f7a6ef936702","metadata":{"id":"cb880382-83f8-4eee-8dcc-f7a6ef936702"},"outputs":[],"source":["print('Gradient function for z =', z.grad_fn)\n","print('Gradient function for loss =', loss.grad_fn)"]},{"cell_type":"markdown","id":"1b5658d1-39bb-4218-a057-8b9af5bf800b","metadata":{"id":"1b5658d1-39bb-4218-a057-8b9af5bf800b"},"source":["### B. 변화도(Gradient) 계산하기\n","신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 도함수(derivative)를 계산해야 합니다. 즉, ``x`` 와 ``y`` 의 일부 고정값에서 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$ 가 필요합니다.\n","\n","이러한 도함수를 계산하기 위해, ``loss.backward()`` 를 호출한 다음 ``w.grad`` 와\n","``b.grad`` 에서 값을 가져옵니다:"]},{"cell_type":"code","execution_count":null,"id":"a704989c-91ff-4c8d-b2f4-7bb4540e76a7","metadata":{"id":"a704989c-91ff-4c8d-b2f4-7bb4540e76a7"},"outputs":[],"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"markdown","id":"5af95bad-529b-4362-9a43-627fb600cbe3","metadata":{"id":"5af95bad-529b-4362-9a43-627fb600cbe3"},"source":["<div class=\"alert alert-info\">\n","    <b>📌NOTE</b>\n","    <div>\n","        <p>• 연산 그래프의 잎(leaf) 노드들 중 <i><b>requires_grad</b></i> 속성이 <i><b>True</b></i> 로 설정된 노드들의 <i><b>grad</b></i> 속성만 구할 수 있습니다. 그래프의 다른 모든 노드에서는 변화도가 유효하지 않습니다.</p>\n","        <p>• 성능 상의 이유로, 주어진 그래프에서의 <i><b>backward</b></i>를 사용한 변화도 계산은 한 번만 수행할 수 있습니다. 만약 동일한 그래프에서 여러번의 <i><b>backward</b></i> 호출이 필요하면, <i><b>backward</b></i> 호출 시에 <i><b>retrain_graph=True</b></i> 를 전달해야 합니다.</p>\n","    </div>\n","</div>"]},{"cell_type":"markdown","id":"8cfbb790-2f37-44e7-86d7-86ff21351942","metadata":{"id":"8cfbb790-2f37-44e7-86d7-86ff21351942"},"source":["### C. 변화도 추적 멈추기"]},{"cell_type":"markdown","id":"1557d146-e7ee-4e42-8fb7-d20f807d22fd","metadata":{"id":"1557d146-e7ee-4e42-8fb7-d20f807d22fd"},"source":["기본적으로, `requires_grad=True`인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을 지원합니다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파 연산만 필요한 경우에는, 이러한 추적이나 지원이 필요없을 수 있습니다. 연산 코드를 `torch.no_grad()` 블록으로 둘러싸서 연산 추적을 멈출 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"10b1c43a-b570-4917-817a-e8003a5118da","metadata":{"id":"10b1c43a-b570-4917-817a-e8003a5118da"},"outputs":[],"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"markdown","id":"8348b424-4424-4c42-8f29-2362a0724c1f","metadata":{"id":"8348b424-4424-4c42-8f29-2362a0724c1f"},"source":["동일한 결과를 얻는 다른 방법은 텐서에 `detach()` 메소드를 사용하는 것입니다.\n","\n","변화도 추적을 멈춰야 하는 이유들은 다음과 같습니다:\n","- 신경망의 일부 매개변수를 고정된 **매개변수(frozen parameter)**로 표시합니다. 이는 [사전 학습된 신경망을 미세조정](https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html) 할 때 매우 일반적인 시나리오입니다.\n","- 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때 **연산 속도가 향상됩니다.**"]},{"cell_type":"markdown","id":"326cedcf-8665-478f-ab0c-d7f7129e25fa","metadata":{"id":"326cedcf-8665-478f-ab0c-d7f7129e25fa"},"source":["### D. 연산 그래프에 대한 추가 정보"]},{"cell_type":"markdown","id":"3ca91d38-5c91-4dd0-8443-ce41423e5fbf","metadata":{"id":"3ca91d38-5c91-4dd0-8443-ce41423e5fbf"},"source":["개념적으로, autograd는 데이터(텐서)의 및 실행된 모든 연산들(및 연산 결과가 새로운 텐서인 경우도 포함하여)의 기록을 [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) 객체로\n","구성된 방향성 비순환 그래프(DAG; Directed Acyclic Graph)에 저장(keep)합니다. 이 방향성 비순환 그래프(DAG)의 잎(leave)은 입력 텐서이고, 뿌리(root)는 결과 텐서입니다. 이 그래프를 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산할 수 있습니다.\n","\n","순전파 단계에서, `autograd`는 다음 두 가지 작업을 동시에 수행합니다:\n","\n","- 요청된 연산을 수행하여 결과 텐서를 계산하고,\n","- DAG에 연산의 *변화도 기능(gradient function)* 를 유지(maintain)합니다.\n","\n","역전파 단계는 DAG 뿌리(root)에서 ``.backward()`` 가 호출될 때 시작됩니다. ``autograd`` 는 이 때:\n","\n","- 각 ``.grad_fn`` 으로부터 변화도를 계산하고,\n","- 각 텐서의 ``.grad`` 속성에 계산 결과를 쌓고(accumulate),\n","- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate)합니다.\n","\n","<div class=\"alert alert-info\">\n","    <b>📌NOTE</b>\n","    <div>\n","  <b>PyTorch에서 DAG들은 동적(dynamic)입니다.</b><br>\n","      주목해야 할 중요한 점은 그래프가 처음부터(from scratch) 다시 생성된다는 것입니다. 매번 <b><i>.bachward()</i></b> 가 호출되고 나면, autograd는 새로운 그래프를 채우기(populate) 시작합니다. 이러한 점 덕분에 모델에서 흐름 제어(control flow) 구문들을 사용할 수 있게 되는 것입니다; 매번 반복(iteration)할 때마다 필요하면 모양(shape)이나 크기(size), 연산(operation)을 바꿀 수 있습니다.\n","    </div>\n","</div>"]},{"cell_type":"markdown","id":"201244df-f973-48db-a7b2-8af96d91ad09","metadata":{"id":"201244df-f973-48db-a7b2-8af96d91ad09"},"source":["### E. 선택적으로 읽기(Optional Reading): 텐서 변화도와 야코비안 곱(Jacobian Product)"]},{"cell_type":"markdown","id":"4049c8c6-ba5f-40cf-a99c-bd301753053d","metadata":{"id":"4049c8c6-ba5f-40cf-a99c-bd301753053d"},"source":["대부분의 경우, 스칼라 손실 함수를 가지고 일부 매개변수와 관련한 변화도를 계산해야 합니다. 그러나 출력 함수가 임의의 텐서인 경우가 있습니다. 이럴 때, PyTorch는 실제 변화도가 아닌 **야코비안 곱(Jacobian product)**\\ 을 계산합니다.\n","\n","$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 이고, $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`$ 일 때 벡터 함수 $\\vec{y}=f(\\vec{x})$에서 $\\vec{x}$에 대한 $\\vec{y}$의 변화도는 **야코비안 행렬(Jacobian matrix)**로 주어집니다:\n","\n","$$J=\\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right) $$\n","\n","야코비안 행렬 자체를 계산하는 대신, PyTorch는 주어진 입력 벡터 $v=(v_1 \\dots v_m)$ 에 대한 **야코비안 곱(Jacobian Product)**  $v^T\\cdot J$ 을 계산합니다.\n","이 과정은 $v$ 를 인자로 ``backward`` 를 호출하면 이뤄집니다. $v$의 크기는 곱(product)을 계산하려고 하는 원래 텐서의 크기와 같아야 합니다."]},{"cell_type":"code","execution_count":null,"id":"e0f36f8f-b5cc-4f06-a23a-a997a143102c","metadata":{"id":"e0f36f8f-b5cc-4f06-a23a-a997a143102c"},"outputs":[],"source":["inp = torch.eye(5, requires_grad=True)\n","out = (inp+1).pow(2)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"First call\\n\", inp.grad)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nSecond call\\n\", inp.grad)\n","inp.grad.zero_()\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(\"\\nCall after zeroing gradients\\n\", inp.grad)"]},{"cell_type":"markdown","id":"2b294856-bafd-4967-bd2e-608830bbdb57","metadata":{"id":"2b294856-bafd-4967-bd2e-608830bbdb57"},"source":["동일한 인자로 `backward`를 두차례 호출하면 변화도 값이 달라집니다. 이는 역방향 전파를 수행할 때, PyTorch가 **변화도를 누적(accumulate)해두기 때문**입니다. 즉, 계산된 변화도의 값이 연산 그래프의 모든 잎(leaf) 노드의 `grad` 속성에 추가됩니다. 따라서 제대로된 변화도를 계산하기 위해서는 `grad` 속성을 먼저 0으로 만들어야 합니다. 실제 학습 과정에서는 *옵티마이저(optimizer)* 가 이 과정을 도와줍니다."]},{"cell_type":"markdown","id":"9fd1d504-c110-46cf-a412-4307aa42529a","metadata":{"id":"9fd1d504-c110-46cf-a412-4307aa42529a"},"source":["<div class=\"alert alert-info\">\n","    <b>📌NOTE</b>\n","    <div>이전에는 매개변수 없이 <b><i>backward()</i></b> 함수를 호출했습니다. 이는 본질적으로 <b><i>backward(torch.tensor(1.0))</i></b>을 호출하는 것과 동일하며, 신경망 훈련 중의 손실과 같은 스칼라-값 함수의 변화도를 계산하는 유용한 방법입니다.\n","    </div>\n","</div>"]},{"cell_type":"markdown","id":"73e8699f-6d35-4efa-9e9a-f4660b6260c0","metadata":{"id":"73e8699f-6d35-4efa-9e9a-f4660b6260c0"},"source":["### F. [실습] Backpropagation with Autograd"]},{"cell_type":"markdown","id":"d8acbc6d-9258-4629-9083-6084a6fb3108","metadata":{"id":"d8acbc6d-9258-4629-9083-6084a6fb3108"},"source":["앞선 실습에서는 신경망의 순전파 단계와 역전파 단계를 직접 구현해보았습니다. 작은 2계층(2-layer) 신경망에서는 역전파 단계를 직접 구현하는 것이 큰일이 아니지만, 복잡한 대규모 신경망에서는 매우 아슬아슬한 일일 것입니다.\n","\n","다행히도, [자동 미분](https://en.wikipedia.org/wiki/Automatic_differentiation) 을 사용하여 신경망의 역전파 단계 연산을 자동화할 수 있습니다. PyTorch의 **autograd** 패키지는 정확히 이런 기능을 제공합니다. Autograd를 사용하면, 신경망의 순전파 단계에서 **연산 그래프(computational graph)** 를 정의하게 됩니다; 이 그래프의 노드(node)는 텐서(tensor)이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를 만들어내는 함수가 됩니다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있습니다.\n","\n","이는 복잡하게 들리겠지만, 실제로 사용하는 것은 매우 간단합니다. 각 텐서는 연산그래프에서 노드로 표현됩니다. 만약 x 가 `x.requires_grad=True` 인 텐서라면 `x.grad` 어떤 스칼라 값에 대한 x 의 변화도를 갖는 또 다른 텐서입니다.\n","\n","이번 단계에서는 텐서 연산을 사용하여 순전파 단계를 계산하고, PyTorch autograd를 사용하여 변화도(gradient)를 계산해보겠습니다.\n","\n","✅ 구현해야 하는 사항 : 이전 구현에서 역전파 부분을 autograd 기능으로 대체한다."]},{"cell_type":"code","execution_count":null,"id":"94484464-1019-4631-aea7-c5c9b2fc662d","metadata":{"id":"94484464-1019-4631-aea7-c5c9b2fc662d"},"outputs":[],"source":["import torch\n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n","\n","# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n","# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n","# 계산할 필요가 없음을 나타냅니다.\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# 가중치를 갖는 임의의 텐서를 생성합니다. 3차 다항식이므로 4개의 가중치가 필요합니다:\n","# y = a + b x + c x^2 + d x^3\n","# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 \n","# 있음을 나타냅니다.\n","a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","\n","print(f'------------------ init ------------------')\n","print(f'x({len(x)}) : {x}')\n","print(f'y({len(y)}) : {y}')\n","print(f'가중치 초기값 : ')\n","print(f'  a : {a}')\n","print(f'  b : {b}')\n","print(f'  c : {c}')\n","print(f'  d : {d}')\n","print(f'------------------------------------------')\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # 순전파 단계: 텐서들 간의 연산을 사용하여 예측값 y를 계산합니다.\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # 텐서들간의 연산을 사용하여 손실(loss)을 계싼하고 출력합니다.\n","    # 이 때 손실은 (1,) shape을 갖는 텐서입니다.\n","    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(f'[t = {t+1:4d}] loss : {loss:.3f}')\n","\n","    # autograd 를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를 갖는\n","    # 모든 텐서들에 대한 손실의 변화도를 계산합니다. \n","    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n","    # 갖는 텐서가 됩니다.\n","    loss.backward()\n","\n","    # 경사하강법(gradient descent)를 사용하여 가중치를 직접 갱신합니다.\n","    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n","    # autograd에서는 이를 추적하지 않을 것이기 때문입니다.\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= learning_rate * d.grad\n","\n","        # 가중치 갱신 후에는 변화도를 직접 0으로 만듭니다.\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = None\n","\n","print(f'------------------------------------------')\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n","\n","with torch.no_grad():\n","    show(x, y, a + b * x + c * x ** 2 + d * x ** 3)"]},{"cell_type":"markdown","id":"56adc9ff-6855-4f95-bea8-0e645905aef3","metadata":{"id":"56adc9ff-6855-4f95-bea8-0e645905aef3"},"source":["<div class=\"alert alert-warning\">\n","    <h4>Q. `loss.backward()`는 어떻게 역전파를 진행하는가?</h4>\n","    <p>👉 가중치에 대한 loss function 의 미분값들을 모두 더해준다.</p>\n","</div>"]},{"cell_type":"markdown","id":"b6ae3779-cd06-4575-9202-c2a4ee69152d","metadata":{"id":"b6ae3779-cd06-4575-9202-c2a4ee69152d"},"source":["## 3. 변형(TRANSFORM)\n","---"]},{"cell_type":"markdown","id":"dd3d6e94-d2bd-49df-94da-22f25f740f91","metadata":{"id":"dd3d6e94-d2bd-49df-94da-22f25f740f91"},"source":["데이터가 항상 머신러닝 알고리즘 학습에 필요한 최종 처리가 된 형태로 제공되지는 않습니다. 변형(transform) 을 해서 데이터를 조작하고 학습에 적합하게 만듭니다.\n","\n","모든 TorchVision 데이터셋들은 변형 로직을 갖는, 호출 가능한 객체(callable)를 받는 매개변수 두개 ( 특징(feature)을 변경하기 위한 transform 과 정답(label)을 변경하기 위한 target_transform )를 갖습니다 [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) 모듈은 주로 사용하는 몇가지 변형(transform)을 제공합니다.\n","\n","FashionMNIST 특징(feature)은 PIL Image 형식이며, 정답(label)은 정수(integer)입니다. 학습을 하려면 정규화(normalize)된 텐서 형태의 특징(feature)과 원-핫(one-hot)으로 부호화(encode)된 텐서 형태의 정답(label)이 필요합니다. 이러한 변형(transformation)을 하기 위해 `ToTensor` 와 `Lambda` 를 사용합니다."]},{"cell_type":"code","execution_count":null,"id":"0efdb21a-48d7-4684-a1d7-ba3f04783750","metadata":{"id":"0efdb21a-48d7-4684-a1d7-ba3f04783750"},"outputs":[],"source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","ds = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n","    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",")\n","\n","ds"]},{"cell_type":"markdown","id":"9f9966e5-afb4-463e-ac78-8efc507892db","metadata":{"id":"9f9966e5-afb4-463e-ac78-8efc507892db"},"source":["### A. ToTensor()"]},{"cell_type":"markdown","id":"6e2449af-b200-4d86-a96a-f257188b244c","metadata":{"id":"6e2449af-b200-4d86-a96a-f257188b244c"},"source":["`ToTensor`는 PIL Image나 NumPy ndarray 를 FloatTensor 로 변환하고, 이미지의 픽셀의 크기(intensity) 값을 [0., 1.] 범위로 비례하여 조정(scale)합니다."]},{"cell_type":"markdown","id":"abc3c41d-b5a4-4140-9f0c-86849c263b79","metadata":{"id":"abc3c41d-b5a4-4140-9f0c-86849c263b79"},"source":["### B. Lambda 변형(Transform)"]},{"cell_type":"markdown","id":"4083339e-b2cd-404f-bc11-3e7313d8f7c2","metadata":{"id":"4083339e-b2cd-404f-bc11-3e7313d8f7c2"},"source":["Lambda 변형은 사용자 정의 람다(lambda) 함수를 적용합니다. 여기에서는 정수를 원-핫으로 부호화된 텐서로 바꾸는 함수를 정의합니다. 이 함수는 먼저 (데이터셋 정답의 개수인) 크기 10짜리 영 텐서(zero tensor)를 만들고, [scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_) 를 호출하여 주어진 정답 y 에 해당하는 인덱스에 `value=1` 을 할당합니다."]},{"cell_type":"code","execution_count":null,"id":"ca365aad-4990-4de6-b120-41ee4f229d44","metadata":{"id":"ca365aad-4990-4de6-b120-41ee4f229d44"},"outputs":[],"source":["target_transform = Lambda(lambda y: torch.zeros(\n","    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"]},{"cell_type":"markdown","id":"-dcGs7jPd_fs","metadata":{"id":"-dcGs7jPd_fs"},"source":["### C. ILLUSTRATION OF TRANSFORMS"]},{"cell_type":"markdown","id":"BfMST1WFg6cW","metadata":{"id":"BfMST1WFg6cW"},"source":["`torchvision.transforms`을 통해서는 다양한 변환들에 대해여 알아봅시다. 여기서에 등장하지 않는 변환들에서 대해서는 [공식문서](https://pytorch.org/vision/stable/transforms.html)에서 확인해 보실 수 있습니다.\n","\n","✅ 예시와 함께 주어진 빈칸을 [공식문서](https://pytorch.org/vision/stable/transforms.html)을 참고하여 각 method에 대한 설명으로 채워보세요."]},{"cell_type":"code","execution_count":null,"id":"BadU3YQyh0k7","metadata":{"id":"BadU3YQyh0k7"},"outputs":[],"source":["from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import urllib.request\n","\n","import torch\n","import torchvision.transforms as T\n","\n","\n","plt.rcParams[\"savefig.bbox\"] = 'tight'\n","plt.rcParams[\"figure.figsize\"] = (14,6)\n","\n","img_url = 'https://images.unsplash.com/photo-1597848212624-a19eb35e2651?w=600'\n","urllib.request.urlretrieve(img_url, \"./sunflower.jpg\")\n","orig_img = Image.open('./sunflower.jpg')\n","plt.imshow(np.asarray(orig_img))\n","\n","# if you change the seed, make sure that the randomly-applied transforms\n","# properly show that the image can be both transformed and *not* transformed!\n","torch.manual_seed(0)\n","\n","\n","def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n","    if not isinstance(imgs[0], list):\n","        # Make a 2d grid even if there's just 1 row\n","        imgs = [imgs]\n","\n","    num_rows = len(imgs)\n","    num_cols = len(imgs[0]) + with_orig\n","    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n","    for row_idx, row in enumerate(imgs):\n","        row = [orig_img] + row if with_orig else row\n","        for col_idx, img in enumerate(row):\n","            ax = axs[row_idx, col_idx]\n","            ax.imshow(np.asarray(img), **imshow_kwargs)\n","            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","    if with_orig:\n","        axs[0, 0].set(title='Original image')\n","        axs[0, 0].title.set_size(8)\n","    if row_title is not None:\n","        for row_idx in range(num_rows):\n","            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n","\n","    plt.tight_layout()"]},{"cell_type":"markdown","id":"RFBkH4Mqj6JV","metadata":{"id":"RFBkH4Mqj6JV"},"source":["- Pad : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"CcboWkMnkEZ6","metadata":{"id":"CcboWkMnkEZ6"},"outputs":[],"source":["added_imgs = [T.Pad(padding=padding)(orig_img) for padding in (3, 10, 30, 50)]\n","plot(padded_imgs)"]},{"cell_type":"markdown","id":"oq0AjRYykzX9","metadata":{"id":"oq0AjRYykzX9"},"source":["- Resize : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"KLTz7KQJk1nt","metadata":{"id":"KLTz7KQJk1nt"},"outputs":[],"source":["resized_imgs = [T.Resize(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n","plot(resized_imgs)"]},{"cell_type":"markdown","id":"vxyyUYqCkzLj","metadata":{"id":"vxyyUYqCkzLj"},"source":["- CenterCrop : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"iqAQOc4klc1-","metadata":{"id":"iqAQOc4klc1-"},"outputs":[],"source":["center_crops = [T.CenterCrop(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n","plot(center_crops)"]},{"cell_type":"markdown","id":"ItWQpQ6OkzIP","metadata":{"id":"ItWQpQ6OkzIP"},"source":["- FiveCrop : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"mC6OxOOGli_n","metadata":{"id":"mC6OxOOGli_n"},"outputs":[],"source":["(top_left, top_right, bottom_left, bottom_right, center) = T.FiveCrop(size=(100, 100))(orig_img)\n","plot([top_left, top_right, bottom_left, bottom_right, center])"]},{"cell_type":"markdown","id":"om0KqV8XkzE0","metadata":{"id":"om0KqV8XkzE0"},"source":["- Grayscale : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"11xtq4Hglo0m","metadata":{"id":"11xtq4Hglo0m"},"outputs":[],"source":["gray_img = T.Grayscale()(orig_img)\n","plot([gray_img], cmap='gray')"]},{"cell_type":"markdown","id":"lJkrTbdokzBW","metadata":{"id":"lJkrTbdokzBW"},"source":["#### Random transforms\n","아래에 method는 랜덤하게 동작하는 변형들 입니다."]},{"cell_type":"markdown","id":"bsHfPW4Uky-V","metadata":{"id":"bsHfPW4Uky-V"},"source":["- ColorJitter : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"l5yAg22smAix","metadata":{"id":"l5yAg22smAix"},"outputs":[],"source":["jitter = T.ColorJitter(brightness=.5, hue=.3)\n","jitted_imgs = [jitter(orig_img) for _ in range(4)]\n","plot(jitted_imgs)"]},{"cell_type":"markdown","id":"pHXFccIrky7A","metadata":{"id":"pHXFccIrky7A"},"source":["- GaussianBlur : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"PJSiiLtgmGgs","metadata":{"id":"PJSiiLtgmGgs"},"outputs":[],"source":["blurrer = T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n","blurred_imgs = [blurrer(orig_img) for _ in range(4)]\n","plot(blurred_imgs)"]},{"cell_type":"markdown","id":"rcr7Hot1ky1y","metadata":{"id":"rcr7Hot1ky1y"},"source":["- RandomPerspective : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"Kzs6Zf_amMUb","metadata":{"id":"Kzs6Zf_amMUb"},"outputs":[],"source":["perspective_transformer = T.RandomPerspective(distortion_scale=0.6, p=1.0)\n","perspective_imgs = [perspective_transformer(orig_img) for _ in range(4)]\n","plot(perspective_imgs)"]},{"cell_type":"markdown","id":"oq4J1xsKkyyq","metadata":{"id":"oq4J1xsKkyyq"},"source":["- RandomRotation : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"_cYoAEMtmRhZ","metadata":{"id":"_cYoAEMtmRhZ"},"outputs":[],"source":["rotater = T.RandomRotation(degrees=(0, 180))\n","rotated_imgs = [rotater(orig_img) for _ in range(4)]\n","plot(rotated_imgs)"]},{"cell_type":"markdown","id":"3IM4MoX9kyvE","metadata":{"id":"3IM4MoX9kyvE"},"source":["- RandomAffine : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"7hm0Iv0BmX3y","metadata":{"id":"7hm0Iv0BmX3y"},"outputs":[],"source":["affine_transfomer = T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))\n","affine_imgs = [affine_transfomer(orig_img) for _ in range(4)]\n","plot(affine_imgs)"]},{"cell_type":"markdown","id":"z6Q8C_qCkysA","metadata":{"id":"z6Q8C_qCkysA"},"source":["- RandomResizedCrop : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"w3_1g-FMmirM","metadata":{"id":"w3_1g-FMmirM"},"outputs":[],"source":["resize_cropper = T.RandomResizedCrop(size=(32, 32))\n","resized_crops = [resize_cropper(orig_img) for _ in range(4)]\n","plot(resized_crops)"]},{"cell_type":"markdown","id":"VOvTc32HmiDm","metadata":{"id":"VOvTc32HmiDm"},"source":["- RandomEqualize : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"mfIDI26Rmyk2","metadata":{"id":"mfIDI26Rmyk2"},"outputs":[],"source":["equalizer = T.RandomEqualize()\n","equalized_imgs = [equalizer(orig_img) for _ in range(4)]\n","plot(equalized_imgs)"]},{"cell_type":"markdown","id":"LjZ7depvmiAS","metadata":{"id":"LjZ7depvmiAS"},"source":["- AutoAugment : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"H_WNYVZPm7gT","metadata":{"id":"H_WNYVZPm7gT"},"outputs":[],"source":["policies = [T.AutoAugmentPolicy.CIFAR10, T.AutoAugmentPolicy.IMAGENET, T.AutoAugmentPolicy.SVHN]\n","augmenters = [T.AutoAugment(policy) for policy in policies]\n","imgs = [\n","    [augmenter(orig_img) for _ in range(4)]\n","    for augmenter in augmenters\n","]\n","row_title = [str(policy).split('.')[-1] for policy in policies]\n","plot(imgs, row_title=row_title)"]},{"cell_type":"markdown","id":"6gnE6jF3mh8q","metadata":{"id":"6gnE6jF3mh8q"},"source":["#### Randomly-applied transforms\n","\n","일부 변환은 확률 p가 주어지면 무작위로 적용됩니다. 즉, 동일한 변환기 인스턴스로 호출하더라도 변환된 이미지가 실제로 원본 이미지와 같을 수 있습니다!"]},{"cell_type":"markdown","id":"dyXHL9rlmh46","metadata":{"id":"dyXHL9rlmh46"},"source":["- RandomHorizontalFlip : `(method의 동작)`\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5BV0151RnJdj","metadata":{"id":"5BV0151RnJdj"},"outputs":[],"source":["hflipper = T.RandomHorizontalFlip(p=0.5)\n","transformed_imgs = [hflipper(orig_img) for _ in range(4)]\n","plot(transformed_imgs)"]},{"cell_type":"markdown","id":"dEPTwfiJmhza","metadata":{"id":"dEPTwfiJmhza"},"source":["- RandomVerticalFlip : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"dgRcAY9mnQSK","metadata":{"id":"dgRcAY9mnQSK"},"outputs":[],"source":["vflipper = T.RandomVerticalFlip(p=0.5)\n","transformed_imgs = [vflipper(orig_img) for _ in range(4)]\n","plot(transformed_imgs)"]},{"cell_type":"markdown","id":"d6ZPfIPZnQ8s","metadata":{"id":"d6ZPfIPZnQ8s"},"source":["- RandomApply : `(method의 동작)`"]},{"cell_type":"code","execution_count":null,"id":"SiG4MCxqnVBU","metadata":{"id":"SiG4MCxqnVBU"},"outputs":[],"source":["applier = T.RandomApply(transforms=[T.RandomCrop(size=(64, 64))], p=0.5)\n","transformed_imgs = [applier(orig_img) for _ in range(4)]\n","plot(transformed_imgs)"]},{"cell_type":"markdown","id":"h6tkfBM6ncR9","metadata":{"id":"h6tkfBM6ncR9"},"source":["### D. Compose"]},{"cell_type":"markdown","id":"q0_IuNvFngjq","metadata":{"id":"q0_IuNvFngjq"},"source":["여러 변환들을 함께 사용하고 싶은 경우 Compose를 이용하여 변환의 파이프라인을 만들 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"gdkBYEbhngDJ","metadata":{"id":"gdkBYEbhngDJ"},"outputs":[],"source":["my_transform = T.Compose([\n","    T.RandomHorizontalFlip(p=0.5),\n","    T.RandomVerticalFlip(p=0.5),\n","    T.RandomRotation(degrees=(0, 180)),\n","    ToTensor()\n"," ])"]},{"cell_type":"code","execution_count":null,"id":"ckY3c-K9oh-w","metadata":{"id":"ckY3c-K9oh-w"},"outputs":[],"source":["ds = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=my_transform,\n","    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",")\n","\n","dataloader = DataLoader(ds, batch_size=9, shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"mgp8D2qPonAP","metadata":{"id":"mgp8D2qPonAP"},"outputs":[],"source":["features, labels = next(iter(dataloader))\n","\n","fig = plt.figure(figsize=(10,10))\n","i = 1\n","\n","for img, label in zip(features, labels):\n","    ax = fig.add_subplot(3, 3, i)\n","    ax.imshow(img.squeeze(), cmap=\"gray\")\n","    ax.set_title(label.numpy())\n","    ax.set_xticks([]), ax.set_yticks([])\n","    i += 1\n","    \n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"220323 day2 pytorch_basic :: solution.ipynb","provenance":[{"file_id":"1ncrwB1uPHnOeTLgwMfDG70Ku3hecjD6g","timestamp":1648021871524}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc-autonumbering":false,"toc-showtags":false},"nbformat":4,"nbformat_minor":5}